{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Embeddings from Pre-trained Nicheformer Model\n",
    "\n",
    "This notebook extracts embeddings from a pre-trained Nicheformer model and stores them in an AnnData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alejandro.tejada/miniforge3/envs/nf_test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import anndata as ad\n",
    "from typing import Optional, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nicheformer.models import Nicheformer\n",
    "from nicheformer.data import NicheformerDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the configuration parameters for the embedding extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path': '/lustre/groups/ml01/projects/2023_nicheformer_data_anna.schaar/spatial/preprocessed/human/nanostring_cosmx_human_liver.h5ad', #'path/to/your/data.h5ad',  # Path to your AnnData file\n",
    "    'technology_mean_path': '/lustre/groups/ml01/projects/2023_nicheformer/data/data_to_tokenize/cosmx_mean_script.npy', #'path/to/technology_mean.npy',  # Path to technology mean file\n",
    "    'checkpoint_path': '/lustre/groups/ml01/projects/2023_nicheformer/pretrained_models/everything_heads_16_blocks_12_maxsteps_30661140_FINAL/epoch=1-step=265000.ckpt',  # Path to model checkpoint\n",
    "    'output_path': 'data_with_embeddings.h5ad',  # Where to save the result, it is a new h5ad\n",
    "    'output_dir': '.',  # Directory for any intermediate outputs\n",
    "    'batch_size': 32,\n",
    "    'max_seq_len': 1500, \n",
    "    'aux_tokens': 30, \n",
    "    'chunk_size': 1000, # to prevent OOM\n",
    "    'num_workers': 4,\n",
    "    'precision': 32,\n",
    "    'embedding_layer': -1,  # Which layer to extract embeddings from (-1 for last layer)\n",
    "    'embedding_name': 'embeddings'  # Name suffix for the embedding key in adata.obsm\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ad.read_h5ad('/lustre/groups/ml01/projects/2023_nicheformer/data/data_to_tokenize/model.h5ad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Load data\n",
    "adata = ad.read_h5ad(config['data_path'])\n",
    "technology_mean = np.load(config['technology_mean_path'])\n",
    "\n",
    "# format data properly with the model\n",
    "adata = ad.concat([model, adata], join='outer', axis=0)\n",
    "# dropping the first observation \n",
    "adata = adata[1:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reference, the metadata tokens are \n",
    "\n",
    "modality_dict = {\n",
    "    'dissociated': 3,\n",
    "    'spatial': 4,}\n",
    "\n",
    "specie_dict = {\n",
    "    'human': 5,\n",
    "    'Homo sapiens': 5,\n",
    "    'Mus musculus': 6,\n",
    "    'mouse': 6,}\n",
    "\n",
    "technology_dict = {\n",
    "    \"merfish\": 7,\n",
    "    \"MERFISH\": 7,\n",
    "    \"cosmx\": 8,\n",
    "    \"NanoString digital spatial profiling\": 8,\n",
    "    \"visium\": 9,\n",
    "    \"10x 5' v2\": 10,\n",
    "    \"10x 3' v3\": 11,\n",
    "    \"10x 3' v2\": 12,\n",
    "    \"10x 5' v1\": 13,\n",
    "    \"10x 3' v1\": 14,\n",
    "    \"10x 3' transcription profiling\": 15, \n",
    "    \"10x transcription profiling\": 15,\n",
    "    \"10x 5' transcription profiling\": 16,\n",
    "    \"CITE-seq\": 17, \n",
    "    \"Smart-seq v4\": 18,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change accordingly\n",
    "\n",
    "adata.obs['modality'] = 4 # spatial\n",
    "adata.obs['specie'] = 5 # human\n",
    "adata.obs['assay'] = 8 #cosmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nicheformer_split' not in adata.obs.columns:\n",
    "    adata.obs['nicheformer_split'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.15s/it]\n",
      "/home/icb/alejandro.tejada/miniforge3/envs/nf_test/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = NicheformerDataset(\n",
    "    adata=adata,\n",
    "    technology_mean=technology_mean,\n",
    "    split='train',\n",
    "    max_seq_len=1500,\n",
    "    aux_tokens=config.get('aux_tokens', 30),\n",
    "    chunk_size=config.get('chunk_size', 1000),\n",
    "    metadata_fields={'obs': ['modality', 'specie', 'assay']}\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config.get('num_workers', 4),\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Set Up Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/alejandro.tejada/miniforge3/envs/nf_test/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/icb/alejandro.tejada/miniforge3/envs/nf_test/l ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "model = Nicheformer.load_from_checkpoint(checkpoint_path=config['checkpoint_path'], strict=False)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Configure trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    default_root_dir=config['output_dir'],\n",
    "    precision=config.get('precision', 32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting embeddings...\")\n",
    "embeddings = []\n",
    "device = model.embeddings.weight.device\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in batch.items()}\n",
    "\n",
    "        # Get embeddings from the model\n",
    "        emb = model.get_embeddings(\n",
    "            batch=batch,\n",
    "            layer=config.get('embedding_layer', -1)  # Default to last layer\n",
    "        )\n",
    "        embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "\n",
    "# Concatenate all embeddings\n",
    "embeddings = np.concatenate(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in AnnData object\n",
    "embedding_key = f\"X_niche_{config.get('embedding_name', 'embeddings')}\"\n",
    "adata.obsm[embedding_key] = embeddings\n",
    "\n",
    "# Save updated AnnData\n",
    "adata.write_h5ad(config['output_path'])\n",
    "\n",
    "print(f\"Embeddings saved to {config['output_path']} in obsm['{embedding_key}']\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
